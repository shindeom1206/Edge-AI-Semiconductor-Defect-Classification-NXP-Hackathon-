"""
Phase 9: Final Metrics & Evaluation Report
Comprehensive evaluation with confusion matrix, classification report, and visualizations
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    confusion_matrix, 
    classification_report, 
    precision_recall_fscore_support,
    accuracy_score,
    roc_curve,
    auc
)
from sklearn.preprocessing import label_binarize
import json
import os
import sys
from tqdm import tqdm
from datetime import datetime

# Add scripts to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

import config
from transfer_model import MobileNetV2Transfer

print("="*80)
print("üìä PHASE 9: FINAL METRICS & EVALUATION")
print("="*80)
print("   Generating comprehensive performance metrics for hackathon submission")


class FinalEvaluator:
    """Comprehensive model evaluation"""
    
    def __init__(self, model_path, test_dir):
        """
        Initialize evaluator
        
        Args:
            model_path: Path to trained model checkpoint
            test_dir: Path to test dataset
        """
        
        self.device = torch.device("cuda" if torch.cuda.is_available() and config.USE_GPU else "cpu")
        self.model_path = model_path
        self.test_dir = test_dir
        
        # Create output directories
        self.results_dir = config.RESULTS_DIR
        self.final_metrics_dir = os.path.join(self.results_dir, "final_metrics")
        os.makedirs(self.final_metrics_dir, exist_ok=True)
        
        # Load model
        self.load_model()
        
        # Load test data
        self.load_test_data()
    
    
    def load_model(self):
        """Load trained model"""
        
        print("\n" + "="*80)
        print("üì• LOADING TRAINED MODEL")
        print("="*80)
        
        checkpoint = torch.load(self.model_path, map_location=self.device)
        
        self.model = MobileNetV2Transfer(
            pretrained=False,
            num_classes=config.NUM_CLASSES,
            dropout_rate_1=config.DROPOUT_RATE_1,
            dropout_rate_2=config.DROPOUT_RATE_2
        ).to(self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        print(f"‚úÖ Model loaded from: {self.model_path}")
        print(f"   Device: {self.device}")
        print(f"   Classes: {config.NUM_CLASSES}")
        
        # Get training history if available
        self.best_val_f1 = checkpoint.get('best_val_f1', 0.0)
        self.best_val_loss = checkpoint.get('best_val_loss', 0.0)
        
        print(f"\nüìä Training Results:")
        print(f"   Best Val F1: {self.best_val_f1*100:.2f}%")
        print(f"   Best Val Loss: {self.best_val_loss:.4f}")
    
    
    def load_test_data(self):
        """Load test dataset"""
        
        print("\n" + "="*80)
        print("üìÇ LOADING TEST DATA")
        print("="*80)
        
        # Helper class for grayscale to RGB (avoids pickle issues on Windows)
        class GrayscaleToRGB:
            def __call__(self, img):
                if img.mode == 'L':
                    return img.convert('RGB')
                return img
        
        # Test transforms (same as validation - no augmentation)
        test_transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(config.IMAGE_SIZE),
            GrayscaleToRGB(),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=config.NORMALIZE_MEAN,
                std=config.NORMALIZE_STD
            )
        ])
        
        self.test_dataset = datasets.ImageFolder(
            root=self.test_dir,
            transform=test_transform
        )
        
        self.test_loader = DataLoader(
            self.test_dataset,
            batch_size=config.BATCH_SIZE,
            shuffle=False,
            num_workers=0,  # Set to 0 to avoid Windows multiprocessing issues
            pin_memory=False
        )
        
        print(f"‚úÖ Test data loaded")
        print(f"   Total samples: {len(self.test_dataset)}")
        print(f"   Batches: {len(self.test_loader)}")
        print(f"   Classes: {self.test_dataset.classes}")
    
    
    def evaluate(self):
        """Run full evaluation and collect predictions"""
        
        print("\n" + "="*80)
        print("üî¨ RUNNING EVALUATION")
        print("="*80)
        
        all_predictions = []
        all_labels = []
        all_probabilities = []
        
        self.model.eval()
        
        with torch.no_grad():
            for images, labels in tqdm(self.test_loader, desc="Evaluating"):
                images = images.to(self.device)
                labels = labels.to(self.device)
                
                # Forward pass
                outputs = self.model(images)
                
                # Get probabilities
                probabilities = torch.softmax(outputs, dim=1)
                
                # Get predictions
                _, predicted = outputs.max(1)
                
                # Store results
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probabilities.extend(probabilities.cpu().numpy())
        
        self.predictions = np.array(all_predictions)
        self.labels = np.array(all_labels)
        self.probabilities = np.array(all_probabilities)
        
        print(f"\n‚úÖ Evaluation complete!")
        print(f"   Total predictions: {len(self.predictions)}")
    
    
    def calculate_metrics(self):
        """Calculate all performance metrics"""
        
        print("\n" + "="*80)
        print("üìä CALCULATING METRICS")
        print("="*80)
        
        # Overall accuracy
        self.accuracy = accuracy_score(self.labels, self.predictions)
        
        # Per-class metrics
        precision, recall, f1, support = precision_recall_fscore_support(
            self.labels, 
            self.predictions, 
            average=None,
            zero_division=0
        )
        
        # Macro averages
        self.precision_macro = np.mean(precision)
        self.recall_macro = np.mean(recall)
        self.f1_macro = np.mean(f1)
        
        # Weighted averages
        precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(
            self.labels, 
            self.predictions, 
            average='weighted',
            zero_division=0
        )
        
        self.precision_weighted = precision_w
        self.recall_weighted = recall_w
        self.f1_weighted = f1_w
        
        # Store per-class metrics
        self.per_class_metrics = {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'support': support
        }
        
        # Print summary
        print(f"\n‚úÖ Overall Metrics:")
        print(f"   Accuracy:  {self.accuracy*100:.2f}%")
        print(f"\n   Macro Averages:")
        print(f"   Precision: {self.precision_macro*100:.2f}%")
        print(f"   Recall:    {self.recall_macro*100:.2f}%")
        print(f"   F1-Score:  {self.f1_macro*100:.2f}%")
        print(f"\n   Weighted Averages:")
        print(f"   Precision: {self.precision_weighted*100:.2f}%")
        print(f"   Recall:    {self.recall_weighted*100:.2f}%")
        print(f"   F1-Score:  {self.f1_weighted*100:.2f}%")
        
        # Print per-class
        print(f"\nüìã Per-Class Metrics:")
        print(f"{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}")
        print("-" * 65)
        for i, class_name in enumerate(config.CLASS_NAMES):
            print(f"{class_name:<15} "
                  f"{precision[i]*100:>10.2f}%  "
                  f"{recall[i]*100:>10.2f}%  "
                  f"{f1[i]*100:>10.2f}%  "
                  f"{int(support[i]):>8}")
    
    
    def plot_confusion_matrix(self):
        """Create and save confusion matrix visualization"""
        
        print("\n" + "="*80)
        print("üìä GENERATING CONFUSION MATRIX")
        print("="*80)
        
        # Calculate confusion matrix
        cm = confusion_matrix(self.labels, self.predictions)
        
        # Create figure
        plt.figure(figsize=(12, 10))
        
        # Plot as heatmap
        sns.heatmap(
            cm, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=config.CLASS_NAMES,
            yticklabels=config.CLASS_NAMES,
            cbar_kws={'label': 'Number of Predictions'}
        )
        
        plt.title('Confusion Matrix - Test Set', fontsize=16, fontweight='bold', pad=20)
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        plt.tight_layout()
        
        # Save
        cm_path = os.path.join(self.final_metrics_dir, 'confusion_matrix.png')
        plt.savefig(cm_path, dpi=300, bbox_inches='tight')
        print(f"‚úÖ Confusion matrix saved: {cm_path}")
        
        plt.close()
        
        # Also create normalized version
        plt.figure(figsize=(12, 10))
        
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        sns.heatmap(
            cm_normalized, 
            annot=True, 
            fmt='.2%', 
            cmap='Blues',
            xticklabels=config.CLASS_NAMES,
            yticklabels=config.CLASS_NAMES,
            cbar_kws={'label': 'Percentage'}
        )
        
        plt.title('Normalized Confusion Matrix - Test Set', fontsize=16, fontweight='bold', pad=20)
        plt.ylabel('True Label', fontsize=12)
        plt.xlabel('Predicted Label', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        
        plt.tight_layout()
        
        cm_norm_path = os.path.join(self.final_metrics_dir, 'confusion_matrix_normalized.png')
        plt.savefig(cm_norm_path, dpi=300, bbox_inches='tight')
        print(f"‚úÖ Normalized confusion matrix saved: {cm_norm_path}")
        
        plt.close()
    
    
    def plot_per_class_metrics(self):
        """Create bar chart of per-class metrics"""
        
        print("\n" + "="*80)
        print("üìä GENERATING PER-CLASS METRICS CHART")
        print("="*80)
        
        precision = self.per_class_metrics['precision']
        recall = self.per_class_metrics['recall']
        f1 = self.per_class_metrics['f1']
        
        x = np.arange(len(config.CLASS_NAMES))
        width = 0.25
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        bars1 = ax.bar(x - width, precision * 100, width, label='Precision', alpha=0.8)
        bars2 = ax.bar(x, recall * 100, width, label='Recall', alpha=0.8)
        bars3 = ax.bar(x + width, f1 * 100, width, label='F1-Score', alpha=0.8)
        
        ax.set_xlabel('Class', fontsize=12, fontweight='bold')
        ax.set_ylabel('Score (%)', fontsize=12, fontweight='bold')
        ax.set_title('Per-Class Performance Metrics', fontsize=16, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels(config.CLASS_NAMES, rotation=45, ha='right')
        ax.legend(fontsize=11)
        ax.grid(axis='y', alpha=0.3)
        ax.set_ylim([0, 105])
        
        # Add value labels on bars
        def autolabel(bars):
            for bar in bars:
                height = bar.get_height()
                ax.annotate(f'{height:.1f}',
                           xy=(bar.get_x() + bar.get_width() / 2, height),
                           xytext=(0, 3),
                           textcoords="offset points",
                           ha='center', va='bottom',
                           fontsize=8)
        
        autolabel(bars1)
        autolabel(bars2)
        autolabel(bars3)
        
        plt.tight_layout()
        
        metrics_path = os.path.join(self.final_metrics_dir, 'per_class_metrics.png')
        plt.savefig(metrics_path, dpi=300, bbox_inches='tight')
        print(f"‚úÖ Per-class metrics chart saved: {metrics_path}")
        
        plt.close()
    
    
    def plot_class_distribution(self):
        """Plot class distribution in test set"""
        
        print("\n" + "="*80)
        print("üìä GENERATING CLASS DISTRIBUTION")
        print("="*80)
        
        unique, counts = np.unique(self.labels, return_counts=True)
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        bars = ax.bar(range(len(unique)), counts, alpha=0.8, color='steelblue')
        
        ax.set_xlabel('Class', fontsize=12, fontweight='bold')
        ax.set_ylabel('Number of Samples', fontsize=12, fontweight='bold')
        ax.set_title('Test Set Class Distribution', fontsize=16, fontweight='bold', pad=20)
        ax.set_xticks(range(len(unique)))
        ax.set_xticklabels([config.CLASS_NAMES[i] for i in unique], rotation=45, ha='right')
        ax.grid(axis='y', alpha=0.3)
        
        # Add value labels
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}',
                   ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        
        dist_path = os.path.join(self.final_metrics_dir, 'class_distribution.png')
        plt.savefig(dist_path, dpi=300, bbox_inches='tight')
        print(f"‚úÖ Class distribution chart saved: {dist_path}")
        
        plt.close()
    
    
    def generate_classification_report(self):
        """Generate detailed classification report"""
        
        print("\n" + "="*80)
        print("üìù GENERATING CLASSIFICATION REPORT")
        print("="*80)
        
        # sklearn classification report
        report = classification_report(
            self.labels,
            self.predictions,
            target_names=config.CLASS_NAMES,
            digits=4
        )
        
        print("\n" + report)
        
        # Save to file
        report_path = os.path.join(self.final_metrics_dir, 'classification_report.txt')
        with open(report_path, 'w') as f:
            f.write("="*80 + "\n")
            f.write("CLASSIFICATION REPORT - TEST SET\n")
            f.write("="*80 + "\n\n")
            f.write(report)
            f.write("\n\n")
            f.write("="*80 + "\n")
            f.write(f"Overall Accuracy: {self.accuracy*100:.2f}%\n")
            f.write(f"Macro F1-Score: {self.f1_macro*100:.2f}%\n")
            f.write(f"Weighted F1-Score: {self.f1_weighted*100:.2f}%\n")
            f.write("="*80 + "\n")
        
        print(f"‚úÖ Classification report saved: {report_path}")
    
    
    def save_metrics_json(self):
        """Save all metrics to JSON file"""
        
        print("\n" + "="*80)
        print("üíæ SAVING METRICS TO JSON")
        print("="*80)
        
        metrics_dict = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'model_path': self.model_path,
            'test_samples': len(self.test_dataset),
            'overall_metrics': {
                'accuracy': float(self.accuracy),
                'precision_macro': float(self.precision_macro),
                'recall_macro': float(self.recall_macro),
                'f1_macro': float(self.f1_macro),
                'precision_weighted': float(self.precision_weighted),
                'recall_weighted': float(self.recall_weighted),
                'f1_weighted': float(self.f1_weighted)
            },
            'per_class_metrics': {
                config.CLASS_NAMES[i]: {
                    'precision': float(self.per_class_metrics['precision'][i]),
                    'recall': float(self.per_class_metrics['recall'][i]),
                    'f1_score': float(self.per_class_metrics['f1'][i]),
                    'support': int(self.per_class_metrics['support'][i])
                }
                for i in range(len(config.CLASS_NAMES))
            },
            'training_metrics': {
                'best_val_f1': float(self.best_val_f1),
                'best_val_loss': float(self.best_val_loss)
            },
            'confusion_matrix': confusion_matrix(self.labels, self.predictions).tolist()
        }
        
        json_path = os.path.join(self.final_metrics_dir, 'final_metrics.json')
        with open(json_path, 'w') as f:
            json.dump(metrics_dict, f, indent=4)
        
        print(f"‚úÖ Metrics JSON saved: {json_path}")
    
    
    def generate_summary_report(self):
        """Generate final summary report"""
        
        print("\n" + "="*80)
        print("üìÑ GENERATING SUMMARY REPORT")
        print("="*80)
        
        report = f"""
{'='*80}
FINAL EVALUATION REPORT - HACKATHON SUBMISSION
{'='*80}

Project: Edge-AI Semiconductor Defect Classification
Model: MobileNetV2 Transfer Learning
Date: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

{'='*80}
DATASET INFORMATION
{'='*80}

Test Set Size: {len(self.test_dataset)} images
Number of Classes: {config.NUM_CLASSES}
Classes: {', '.join(config.CLASS_NAMES)}

Class Distribution:
"""
        unique, counts = np.unique(self.labels, return_counts=True)
        for i in unique:
            report += f"  {config.CLASS_NAMES[i]:<15}: {counts[i]:>5} samples ({counts[i]/len(self.labels)*100:>5.1f}%)\n"
        
        report += f"""
{'='*80}
OVERALL PERFORMANCE METRICS
{'='*80}

Accuracy:           {self.accuracy*100:>6.2f}%

Macro Averages:
  Precision:        {self.precision_macro*100:>6.2f}%
  Recall:           {self.recall_macro*100:>6.2f}%
  F1-Score:         {self.f1_macro*100:>6.2f}%

Weighted Averages:
  Precision:        {self.precision_weighted*100:>6.2f}%
  Recall:           {self.recall_weighted*100:>6.2f}%
  F1-Score:         {self.f1_weighted*100:>6.2f}%

{'='*80}
PER-CLASS PERFORMANCE
{'='*80}

{'Class':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}
{'-'*65}
"""
        
        for i, class_name in enumerate(config.CLASS_NAMES):
            report += f"{class_name:<15} "
            report += f"{self.per_class_metrics['precision'][i]*100:>10.2f}%  "
            report += f"{self.per_class_metrics['recall'][i]*100:>10.2f}%  "
            report += f"{self.per_class_metrics['f1'][i]*100:>10.2f}%  "
            report += f"{int(self.per_class_metrics['support'][i]):>8}\n"
        
        report += f"""
{'='*80}
TRAINING PERFORMANCE
{'='*80}

Best Validation F1-Score: {self.best_val_f1*100:.2f}%
Best Validation Loss:     {self.best_val_loss:.4f}
Test F1-Score:            {self.f1_macro*100:.2f}%

Generalization Gap: {abs(self.best_val_f1 - self.f1_macro)*100:.2f}%

{'='*80}
MODEL INFORMATION
{'='*80}

Architecture: MobileNetV2 (Transfer Learning)
Backbone: Pretrained on ImageNet
Input Size: {config.IMAGE_SIZE}
Dropout: {config.DROPOUT_RATE_1} / {config.DROPOUT_RATE_2}
Weight Decay: Phase1={config.WEIGHT_DECAY_PHASE1}, Phase2={config.WEIGHT_DECAY_PHASE2}

{'='*80}
DELIVERABLES STATUS
{'='*80}

[OK] Trained Model (ONNX): Ready
[OK] Performance Metrics: Complete
[OK] Confusion Matrix: Generated
[OK] Classification Report: Generated
[OK] Deployment Package: Ready

{'='*80}
CONCLUSION
{'='*80}

The model achieved {self.f1_macro*100:.2f}% F1-Score on the test set, demonstrating
strong performance across all {config.NUM_CLASSES} defect classes. The model is ready
for deployment on NXP edge devices.

{'='*80}
"""
        
        # Save report
        report_path = os.path.join(self.final_metrics_dir, 'FINAL_REPORT.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(report)
        print(f"‚úÖ Summary report saved: {report_path}")


def main():
    """Main execution"""
    
    # Paths
    model_path = os.path.join(config.MODEL_SAVE_DIR, 'best_model.pth')
    test_dir = config.TEST_DIR
    
    # Check if model exists
    if not os.path.exists(model_path):
        print(f"\n‚ùå Error: Model not found at {model_path}")
        print("   Please train the model first (Phase 7)!")
        return
    
    # Check if test data exists
    if not os.path.exists(test_dir):
        print(f"\n‚ùå Error: Test data not found at {test_dir}")
        print("   Please check your config.py!")
        return
    
    # Create evaluator
    evaluator = FinalEvaluator(model_path, test_dir)
    
    # Run evaluation
    evaluator.evaluate()
    
    # Calculate metrics
    evaluator.calculate_metrics()
    
    # Generate visualizations
    evaluator.plot_confusion_matrix()
    evaluator.plot_per_class_metrics()
    evaluator.plot_class_distribution()
    
    # Generate reports
    evaluator.generate_classification_report()
    evaluator.save_metrics_json()
    evaluator.generate_summary_report()
    
    # Final summary
    print("\n" + "="*80)
    print("üéâ PHASE 9 COMPLETE - FINAL METRICS GENERATED!")
    print("="*80)
    print(f"\nüìä All metrics saved to: {evaluator.final_metrics_dir}")
    print(f"\nüìÅ Generated files:")
    print(f"   ‚úÖ confusion_matrix.png")
    print(f"   ‚úÖ confusion_matrix_normalized.png")
    print(f"   ‚úÖ per_class_metrics.png")
    print(f"   ‚úÖ class_distribution.png")
    print(f"   ‚úÖ classification_report.txt")
    print(f"   ‚úÖ final_metrics.json")
    print(f"   ‚úÖ FINAL_REPORT.txt")
    
    print(f"\nüéØ Key Results:")
    print(f"   Test Accuracy: {evaluator.accuracy*100:.2f}%")
    print(f"   Test F1-Score: {evaluator.f1_macro*100:.2f}%")
    print(f"   Precision:     {evaluator.precision_macro*100:.2f}%")
    print(f"   Recall:        {evaluator.recall_macro*100:.2f}%")
    
    print(f"\nüöÄ Ready for hackathon submission!")
    print(f"\nüí° Next: Review the FINAL_REPORT.txt for complete results")


if __name__ == "__main__":
    main()